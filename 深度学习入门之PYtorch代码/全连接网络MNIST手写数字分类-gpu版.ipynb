{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的三层全连接神经网络\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, n_hidden_1)\n",
    "        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.layer3 = nn.Linear(n_hidden_2, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "# 添加激活函数\n",
    "class Activation_Net(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(Activation_Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, n_hidden_1), nn.ReLU(True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1, n_hidden_2), nn.ReLU(True)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "# 添加批标准化(加快收敛速度)\n",
    "class Batch_Net(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(Batch_Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, n_hidden_1),\n",
    "            nn.BatchNorm1d(n_hidden_1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            nn.BatchNorm1d(n_hidden_2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.layer3 = nn.Linear(n_hidden_2, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "num_epoches = 5\n",
    "\n",
    "# 数据预处理\n",
    "data_tf = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读出训练数据\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=data_tf\n",
    ")\n",
    "\n",
    "# 从文件中读出测试数据\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=data_tf\n",
    ")\n",
    "\n",
    "# 构造迭代器DataLoader，每次会自动读出batch_size量的数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型，损失函数，优化器\n",
    "if torch.cuda.is_available():\n",
    "    model = [simpleNet(28 * 28, 300, 100, 10).cuda(), Activation_Net(28 * 28, 300, 100, 10).cuda(), Batch_Net(28 * 28, 300, 100, 10).cuda()]\n",
    "else:\n",
    "    model = [simpleNet(28 * 28, 300, 100, 10), Activation_Net(28 * 28, 300, 100, 10), Batch_Net(28 * 28, 300, 100, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程\n",
    "def train(net, train_data, valid_data, num_epoch, optimizer, criterion):\n",
    "    print(net)\n",
    "    length = len(train_data)\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        net = net.train()\n",
    "        for iter, data in enumerate(train_data):\n",
    "            im, label = data\n",
    "            im = im.view(im.size()[0], -1)\n",
    "            if torch.cuda.is_available():\n",
    "                im = Variable(im).cuda() # 转化成可以进行训练的张量\n",
    "                # print(im.shape)\n",
    "                label = Variable(label).cuda()\n",
    "            else:\n",
    "                im = Variable(im) # 转化成可以进行训练的张量\n",
    "                # print(im.shape)\n",
    "                label = Variable(label)\n",
    "            output = net(im)    # 执行一次前向传播，得到网络的输出\n",
    "#             output = output.cuda().data.cpu().numpy()\n",
    "            loss = criterion(output, label) # 计算网络的输出和真实值的损失\n",
    "            # ------------------- 优化过程\n",
    "            optimizer.zero_grad()   # 把梯度变为0\n",
    "            loss.backward() # 执行一次反向传播\n",
    "            optimizer.step()    # 进行一次优化\n",
    "            # -------------------\n",
    "#             _, pred_label = torch.max(output.data, 1)\n",
    "            _, pred_label = torch.max(output.data.cpu(), 1)\n",
    "#              = output.cuda().data.cpu().numpy()\n",
    "            train_loss += loss.data\n",
    "            temp_loss = loss.data\n",
    "\n",
    "            train_acc += accuracy_score(label.data.cpu(), pred_label) * label.size(0) # 累计计算预测的准确率\n",
    "\n",
    "            temp_acc = accuracy_score(label.data.cpu(), pred_label)   # 计算预测的准确率\n",
    "            if iter % 300 == 0 and iter > 0:\n",
    "                print('Epoch {}/{},Iter {}/{} Loss: {:.4f},ACC:{:.4f}' \\\n",
    "                      .format(epoch, num_epoches - 1, iter, length, temp_loss, temp_acc))\n",
    "        if valid_data is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for iter, data in enumerate(valid_data):\n",
    "                    im, label = data\n",
    "                    im = im.view(im.size()[0], -1)\n",
    "                    if torch.cuda.is_available():\n",
    "                        im = Variable(im).cuda() \n",
    "                        label = Variable(label).cuda()\n",
    "                    else:\n",
    "                        im = Variable(im) \n",
    "                        label = Variable(label)\n",
    "                    output = net(im)\n",
    "#                     output = output.cuda().data.cpu().numpy()\n",
    "#                     _, pred_label = torch.max(output.data, 1)\n",
    "#                     _, pred_label = torch.max(output.cuda().data.cpu().numpy(), 1)\n",
    "                    _, pred_label = torch.max(output.data.cpu(), 1)\n",
    "                    loss = criterion(output, label)\n",
    "                    valid_loss += loss.data\n",
    "                    # valid_acc += torch.sum(pred_label == label.data)\n",
    "                    valid_acc += accuracy_score(label.data.cpu(), pred_label) * label.size(0)\n",
    "            print('Epoch {}/{},complete! train_loss: {:.4f},train_acc:{:.4f}' \\\n",
    "                  .format(epoch, num_epoches - 1, train_loss, train_acc / 60000),\n",
    "                  'valid_loss: {:.4f},valid_acc:{:.4f}'.format(valid_loss, valid_acc / 10000)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the simpleNet start traing...\n",
      "simpleNet(\n",
      "  (layer1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 0/4,Iter 300/938 Loss: 0.2415,ACC:0.9219\n",
      "Epoch 0/4,Iter 600/938 Loss: 0.3311,ACC:0.9062\n",
      "Epoch 0/4,Iter 900/938 Loss: 0.1776,ACC:0.9531\n",
      "Epoch 0/4,complete! train_loss: 443.0319,train_acc:0.8572 valid_loss: 61.6233,valid_acc:0.8754\n",
      "Epoch 1/4,Iter 300/938 Loss: 0.2338,ACC:0.9062\n",
      "Epoch 1/4,Iter 600/938 Loss: 0.4442,ACC:0.8750\n",
      "Epoch 1/4,Iter 900/938 Loss: 0.1716,ACC:0.9688\n",
      "Epoch 1/4,complete! train_loss: 332.0761,train_acc:0.8956 valid_loss: 70.9934,valid_acc:0.8710\n",
      "Epoch 2/4,Iter 300/938 Loss: 0.6485,ACC:0.8594\n",
      "Epoch 2/4,Iter 600/938 Loss: 0.3069,ACC:0.9219\n",
      "Epoch 2/4,Iter 900/938 Loss: 0.2780,ACC:0.9219\n",
      "Epoch 2/4,complete! train_loss: 317.3036,train_acc:0.9013 valid_loss: 54.3566,valid_acc:0.8975\n",
      "Epoch 3/4,Iter 300/938 Loss: 0.2984,ACC:0.8906\n",
      "Epoch 3/4,Iter 600/938 Loss: 0.2361,ACC:0.9531\n",
      "Epoch 3/4,Iter 900/938 Loss: 0.3684,ACC:0.8750\n",
      "Epoch 3/4,complete! train_loss: 307.1740,train_acc:0.9043 valid_loss: 58.5068,valid_acc:0.8959\n",
      "Epoch 4/4,Iter 300/938 Loss: 0.3498,ACC:0.9375\n",
      "Epoch 4/4,Iter 600/938 Loss: 0.3520,ACC:0.8906\n",
      "Epoch 4/4,Iter 900/938 Loss: 0.3746,ACC:0.9219\n",
      "Epoch 4/4,complete! train_loss: 302.3203,train_acc:0.9070 valid_loss: 53.4289,valid_acc:0.9016\n",
      "the simpleNet complete traing...\n",
      "the Activation_Net start traing...\n",
      "Activation_Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0/4,Iter 300/938 Loss: 0.2494,ACC:0.9219\n",
      "Epoch 0/4,Iter 600/938 Loss: 0.4827,ACC:0.8750\n",
      "Epoch 0/4,Iter 900/938 Loss: 0.1701,ACC:0.9688\n",
      "Epoch 0/4,complete! train_loss: 402.2421,train_acc:0.8627 valid_loss: 25.0674,valid_acc:0.9511\n",
      "Epoch 1/4,Iter 300/938 Loss: 0.0614,ACC:0.9844\n",
      "Epoch 1/4,Iter 600/938 Loss: 0.1091,ACC:0.9531\n",
      "Epoch 1/4,Iter 900/938 Loss: 0.1383,ACC:0.9531\n",
      "Epoch 1/4,complete! train_loss: 150.7300,train_acc:0.9509 valid_loss: 22.8157,valid_acc:0.9532\n",
      "Epoch 2/4,Iter 300/938 Loss: 0.2126,ACC:0.9531\n",
      "Epoch 2/4,Iter 600/938 Loss: 0.1914,ACC:0.9531\n",
      "Epoch 2/4,Iter 900/938 Loss: 0.1012,ACC:0.9688\n",
      "Epoch 2/4,complete! train_loss: 107.0333,train_acc:0.9651 valid_loss: 15.4388,valid_acc:0.9681\n",
      "Epoch 3/4,Iter 300/938 Loss: 0.1230,ACC:0.9531\n",
      "Epoch 3/4,Iter 600/938 Loss: 0.0770,ACC:0.9688\n",
      "Epoch 3/4,Iter 900/938 Loss: 0.1140,ACC:0.9531\n",
      "Epoch 3/4,complete! train_loss: 83.2231,train_acc:0.9725 valid_loss: 31.9022,valid_acc:0.9279\n",
      "Epoch 4/4,Iter 300/938 Loss: 0.0409,ACC:0.9844\n",
      "Epoch 4/4,Iter 600/938 Loss: 0.0088,ACC:1.0000\n",
      "Epoch 4/4,Iter 900/938 Loss: 0.0365,ACC:0.9844\n",
      "Epoch 4/4,complete! train_loss: 68.3069,train_acc:0.9770 valid_loss: 13.5231,valid_acc:0.9730\n",
      "the Activation_Net complete traing...\n",
      "the Batch_Net start traing...\n",
      "Batch_Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
      "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 0/4,Iter 300/938 Loss: 0.2766,ACC:0.8750\n",
      "Epoch 0/4,Iter 600/938 Loss: 0.1703,ACC:0.9688\n",
      "Epoch 0/4,Iter 900/938 Loss: 0.0961,ACC:0.9531\n",
      "Epoch 0/4,complete! train_loss: 192.1423,train_acc:0.9428 valid_loss: 14.8094,valid_acc:0.9698\n",
      "Epoch 1/4,Iter 300/938 Loss: 0.2441,ACC:0.9219\n",
      "Epoch 1/4,Iter 600/938 Loss: 0.1508,ACC:0.9688\n",
      "Epoch 1/4,Iter 900/938 Loss: 0.0818,ACC:0.9688\n",
      "Epoch 1/4,complete! train_loss: 80.9548,train_acc:0.9743 valid_loss: 12.0325,valid_acc:0.9753\n",
      "Epoch 2/4,Iter 300/938 Loss: 0.1052,ACC:0.9688\n",
      "Epoch 2/4,Iter 600/938 Loss: 0.1142,ACC:0.9688\n",
      "Epoch 2/4,Iter 900/938 Loss: 0.0858,ACC:0.9688\n",
      "Epoch 2/4,complete! train_loss: 54.0746,train_acc:0.9828 valid_loss: 10.9664,valid_acc:0.9771\n",
      "Epoch 3/4,Iter 300/938 Loss: 0.0335,ACC:0.9844\n",
      "Epoch 3/4,Iter 600/938 Loss: 0.0574,ACC:0.9844\n",
      "Epoch 3/4,Iter 900/938 Loss: 0.0616,ACC:0.9844\n",
      "Epoch 3/4,complete! train_loss: 40.6933,train_acc:0.9866 valid_loss: 11.6557,valid_acc:0.9778\n",
      "Epoch 4/4,Iter 300/938 Loss: 0.0065,ACC:1.0000\n",
      "Epoch 4/4,Iter 600/938 Loss: 0.0439,ACC:0.9688\n",
      "Epoch 4/4,Iter 900/938 Loss: 0.0288,ACC:1.0000\n",
      "Epoch 4/4,complete! train_loss: 31.3497,train_acc:0.9895 valid_loss: 11.0700,valid_acc:0.9784\n",
      "the Batch_Net complete traing...\n"
     ]
    }
   ],
   "source": [
    "for m in model:\n",
    "    print(\"the {} start traing...\".format(m.get_name()))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(m.parameters(), 1e-1)  # 使用随机梯度下降，学习率 0.1\n",
    "    train(m, train_loader, test_loader, num_epoches, optimizer, criterion)\n",
    "    print(\"the {} complete traing...\".format(m.get_name()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-gpu2] *",
   "language": "python",
   "name": "conda-env-pytorch-gpu2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
